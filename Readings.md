# Readings Notebook
### Perception Algorithm
#### Lidar and camera
##### [Multi-Modal Fusion Transformer for End-to-End Autonomous Driving](https://arxiv.org/pdf/2104.09224.pdf)
> End to End waypoints prediction

This paper provides a method to use transformer as fusion machine to fuse lidar and camera signals. Lidar BEV and camera image are processed by two ResNet. After each convolutional layer, feature maps of these two inputs will be taken out and divided into several vectors. Then, these vectors will be inputted into a transformer. The transformer will output several vectors, and they are reshaped into two feature maps and putted back to ResNets. At the end of these two ResNets, the resulted feature maps will be added and a RNN is ultilized to predict waypoints.<br>
[Network structure](./pictures/7.png)

##### [Deep Continuous Fusion for Multi-Sensor 3DObject Detection](https://arxiv.org/pdf/2012.10992.pdf)
>3D Object detection.

The author use two ResNet to extract feature maps from Lidar BEV and camera image. All feature maps of the image ResNet will be used to fused togather by a [feature pyramid network](https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf) and go through a continuous fusion layer to output into BEV space. Then, these features will be combined with BEV ResNet feature maps using FPN and then be putted into a Detection Header for final result. See the whole structure: [Network structure](./pictures/3.png)<br>
One important thing in this algorithm is the continuous fusion layer. It is used to project image feature maps into BEV space. Given a pixel in BEV space, it extract K nearest LIDAR points and project them to camera image. With these points, the corresponding image features can be found, and finally a MLP will be used to generate features for that pixel. See this layer at [Continuous fusion layer](./pictures/4.png)<br>
[Network structure](./pictures/5.jpg)


#### Lidar only
##### [PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection](https://arxiv.org/pdf/1912.13192.pdf)
> 3D object detection, one stage.

There are two major methods used in Lidar point cloud object detection: point based and voxel based. This paper combines these two methods togather, and can divided into two stages. In the first stage, the raw cloud points are voxelized and putted into a 3D sparse convolutional neural network. The result of this 3D sparse CNN will be passed into a RPN to regress bounding box and classification.<br>
Meanwhile, Furthest Point-Sampling (FPS) will be used to generate key points. For each key points, Voxel Set Abstraction Module (VSA) will be used to extract features from feature maps generated by 3D Sparse COnvolution. After that, these key points and the bounding box proposals from PRN will be processed by ROI-grid pooling layers to learn proposal specific features and a FCN will be used to regress confidence and box refinement.<br>
[Network structure](./pictures/8.png)

##### [Center-based 3D Object Detection and Tracking](https://arxiv.org/pdf/2006.11275.pdf)
> 3D object detection, same author as CenterNet and same idea. This model is very simple, but I have seen many impoved algorithms based on it in various leader boards.

The key idea of this algorithm is very similar to CenterNet. Concretely, it uses backbone network to generate map-view features, and use a header (2D CNN) to predict heatmaps for each categories and other variables like bounding box and velocity (similar to CenterNet). For training, the author projects 3D bounding box into 2D map-view Gaussian Distribution ground truth and use facol loss. After the header, a MLP will be used to predict scores (confidence) and bounding box refinement.<br>
[Network structure](./pictures/11.png)

#### Camera only
##### [3D Bounding Box Estimation Using Deep Learning and Geometry](https://arxiv.org/pdf/1612.00496.pdf)
>3D obeject detection.

The author proposes an algorithm which project 2D bounding box to 3D dimension by using transfer function of camera. For projection formula, we have *x<sub>2D</sub>*=k*\[R T\]*x<sub>3D</sub>*, where k is the internal reference and \[R T\] is the external reference of the camera. Therefore, if a 3D bounding box has size of (x',y',z') , we have: <br>
(x<sub>min</sub>, y<sub>min</sub>)=k*\[R T\](x'/2,y'/2,z'/2)<br>
In this equation, R is rotation (in R<sup>1</sup>, since pitch and roll could be omitted for a car), T is translation (in R<sup>3</sup>) and the 3D coordinate is (x',y',z'), which is also in R<sup>3</sup>. Assume that the internal reference k and 2D bounding box is known, this equation leaves us with 7 unkown variables. We could find 4 constraints like this, so we need to reduce variables.<br>
Therefore, the author uses a neural network to predict size (in R<sup>3</sup>) and angle, which means that only 3 variables remain, which could be obtained by solving constraint equations.<br>
[Network structure](./pictures/1.png)

##### [Objects as Points](https://arxiv.org/pdf/1904.07850.pdf)
> 2D Object detection and 3D Object detection. This algorithm is also called CenterNet, one stage.<br>

For an image I in R<sup>W\*H\*3</sup>, the model will output keypoint heatmap Y<sub>e</sub> in \[0,1\] in R<sup>W/n\*H/n\*C</sup>. For 2D object detetion, c is object category. For 3D object detection, it should add depth and angle. Use [in-bin regression](https://arxiv.org/pdf/1612.00496.pdf) for orientation. The model is very simple, which combines ResNet and DLA.<br>
To train the model, the author construct a Gaussian Distribution Y<sub>gt</sub> for every object to describe the confidence of that object in any point of that image. The center of one distribution is at the center of that ground truth. If overlap, the max element will be used. For loss function, the author wants Y to estimate the center point. Therefore, if Y<sub>gt</sub> is not 1 at that point, the author wants Y<sub>e</sub> to be 0. It uses facol loss function. See loss function: [Loss function](./pictures/2.png)<br>

##### [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497.pdf)
> 2D object detection, two stage

Faster R-CNN is an anchor based 2D object detection algorithm. For an input image, it resize and normalize it into a fixed size. Then, in the first stage, a convolutional neural network is applied to the resized image. Notice that the convoluitonal layer is padded, so that the image will be downsampled only in pooling layer. The resulted feature map will be putted into a RPN, which predict positive/negative and bounding box regression for each anchor directly. The output of RPN will be processed in proposal layer, which takes a certain number of positive anchors according to scores, and do NMS. In the second stage, a ROI pooling layer will be used to process original feature map and the proposed bounding box. Then, for each bounding box, MLP is used to predict category and do bounding box regression.<br>
[Network structure](./pictures/5.jpg)

##### [Monocular Quasi-Dense 3D Object Tracking](https://arxiv.org/pdf/2103.07351.pdf)
> 3D object tracking

For N trajectories T={t<sup>1</sup>,t<sup>2</sup>,...,t<sup>n</sup>}, t<sup>i</sup><sub>a,b</sub>={s<sup>(i)</sup><sub>a</sub>,s<sup>(i)</sup><sub>a+1</sub>,...,s<sup>(i)</sup><sub>b</sub>}. s<sup>(i)</sup><sub>a</sub> contains position information, including 3D location, 3D size, angle, reference feature and velocity in 3D. Key idea is to find a similar object in adjacent frames, making sure that their features are as similar as possible. The author also proposes data association, which associate s<sup>(i)</sup><sub>a</sub> to a trajectory t<sup>i</sup>.<br>
[Network structure](./pictures/6.png)

##### [AN IMAGE IS WORTH 16 X 16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929.pdf)
> Image classification

The author aims at maintaining the original structure of Transformer in NLP. Therefore, the author proposes to process an image into several vectors, and use the token output of the Transformer as features for regression.<br>
Concretely, an image is divided into several patches. For each patch, it is linearly projected into a vector. These vectors are also applied positional embedding to contains position information. Then, several encoders are used to process these features, and the final output token vector will be used as input features of a MLP to regress classification. Notice that all projection and embedding parameters in this model is learnable. THe author implements it through a 1X1 convolutional layer.<br>
[Network structure](./pictures/9.png)

##### [CornerNet: Detecting Objects as Paired Keypoints](https://arxiv.org/pdf/1808.01244.pdf)
> 2D object detection, one stage.

The author detects an object by their top-left corner and bottom-right corner of the bounding box. For an image, a CNN (Hourglass Network in this algorithm) is used to predict two heatmaps, one is for the top-left corner and one is for the bottom-right corner. Also, this network will generate feature vectors and offsets (to slightly adjust corner location) for these two corners. Corner pooling layer is used in prediction of these three outputs. To train this model, the author uses Gaussian Distribution to simulate heatmaps ground truth.<br>
[Network structure](./pictures/10.png)

##### [End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872.pdf)
> 2D object detection, one stage. This is a very simple model and can be implement within 50 lines using pytorch. Also called DETR.

For an image, the author use CNN backbone to extract features, and use a transformer (contains both encoder and decoder) to directly regress bounding box and classification. The whole model is very simple, but its loss is complex. The output of this model is a set of items, each item contains bounding box (coordinate) and classification. To give the most similar bounding box the correct ground truth, the author uses Hungary algorithm to match predictions and ground truth (Notice that HUngary algorithm can only guarantee local optimal solution). When training bounding box, only those predictions which have matched ground truth participate in training. See loss [LOSS](./pictures/14.png)<br>
[Network structure](./pictures/13.png)

### Behavior Decision
##### [DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving](https://openaccess.thecvf.com/content_iccv_2015/papers/Chen_DeepDriving_Learning_Affordance_ICCV_2015_paper.pdf)
> Affordance indicator and behavior decision.

The author proposes a deep learning method to map an image to affordance indicators (like angle, distance from lines or distance from preceding cars). Dataset comes from KITTI and TORCS. Then, the author decides a control system to compute action spaces by considering velocity and distance. The controller logic could be concluded as: 1. CNN outputs affordance indicators; 2. check availability of both lanes; 3. if approaching preceding car: checking lane changing allowable. If yes, lane changing, slowing down otherwise; 4. if normal driving: follow center line. If lane changing: follow obejctive lane center line; 5. Compute steering command and desired_speed; 6. Control acceleration and brake.<br>
[Network structure](./pictures/15.png)

##### [CoBERL: Contrastive BERT for Reinforcement Learning](https://arxiv.org/pdf/2107.05431.pdf)
> RL algorithms for games like Atari.

The author wants to improve learning efficiency of reinforcement learning with transformer. It is well know that the dataset for reinforcement learning is often small and highly correlated, while transformer requires a large amount of dataset for training. In this paper, the author uses contrastive learning and applies RELIC contrastive losses by adapting it into time domain. To create the data groupings by aligning the input and output of GTrXL Transformer.<br>
In detail, the input of the contrastive proxy (transformer) and the downstream RL work are related but not identical. Therefore, the author proposes to use a critic for seperation (notice that this critic is learnable). For the whole architecture, a encoder will process raw input, and then produce an output Y. Y will be passed into a transformer and generates X. Then, a gate will combines X and Y to generate input for a single LSTM. This whole network is consider as the loss computation of a RL agent (notice that it is in time domain), so it can be used to replace LSTM in some RL algorithms. For example, for R2D2, its LSTM network can be replaced by this CoBERL. <br>
[Network structure](./pictures/16.png)

##### [Probabilistic Decision-Making under Uncertainty for Autonomous Driving using Continuous POMDPs](https://www.researchgate.net/publication/267040968_Probabilistic_Decision-Making_under_Uncertainty_for_Autonomous_Driving_using_Continuous_POMDPs)
> RL algorithm

In this paper, the author models the behavior decision problem in autonomous driving into a continuous POMDP and use a continuous POMDP solver to find an approximate optimal solution. 
[Network structure](./pictures/17.png)

### Loss and others
##### [IoU Loss for 2D/3D Object Detection](https://arxiv.org/pdf/1908.03851.pdf)
> IOU loss for 2D and 3D object detection. Notice that this paper doesn't provide a method to calculate gradient of 3D IOU.

In this paper, the author proposes GIoU loass function: GIoU = IoU- (Area<sub>C</sub>-U)/(Area<sub>C</sub>), where U is the numerator of 1-IoU. See [GIoU Loss](./pictures/12.png).<br>
From my point of view, the reason why 3D IoU loss is not applied in Deep Learning algorithm is that 3D IoU loss is very hard to differentiate. The overlap of two rotated rectange can be a convex polygon with at most 7 edges. Through we do have some techniques to calculate it (for example, we can choose the center point and divided the polygon into sevral triangles based on that point, and then use triangl area function to calculate the total area), the differential machine of Pytorch or Tensorflow don't have internal function to calculate its gradient. This means that we need to compute its gradient function by ourselves, and I think it is a very hard problem. That's why nobody uses 3D IoU loss in deep learning. Back to this paper, the author omits the part of backward (gradient computation) and says that "we implement the backward operations for all these functions and we will make the source code public in the future." These codes are not published yet.

